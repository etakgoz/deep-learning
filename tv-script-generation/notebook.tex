
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dlnd\_tv\_script\_generation}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{TV Script Generation}\label{tv-script-generation}

In this project, you'll generate your own
\href{https://en.wikipedia.org/wiki/The_Simpsons}{Simpsons} TV scripts
using RNNs. You'll be using part of the
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{Simpsons
dataset} of scripts from 27 seasons. The Neural Network you'll build
will generate a new TV script for a scene at
\href{https://simpsonswiki.com/wiki/Moe's_Tavern}{Moe's Tavern}. \#\#
Get the Data The data is already provided for you. You'll be using a
subset of the original dataset. It consists of only the scenes in Moe's
Tavern. This doesn't include other versions of the tavern, like "Moe's
Cavern", "Flaming Moe's", "Uncle Moe's Family Feed-Bag", etc..

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        
        \PY{n}{data\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/simpsons/moes\PYZus{}tavern\PYZus{}lines.txt}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{text} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Ignore notice, since we don\PYZsq{}t use it for analysing the data}
        \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{p}{[}\PY{l+m+mi}{81}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \subsection{Explore the Data}\label{explore-the-data}

Play around with \texttt{view\_sentence\_range} to view different parts
of the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{view\PYZus{}sentence\PYZus{}range} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset Stats}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Roughly the number of unique words: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{word}\PY{p}{:} \PY{k+kc}{None} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{scenes} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of scenes: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scenes}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{sentence\PYZus{}count\PYZus{}scene} \PY{o}{=} \PY{p}{[}\PY{n}{scene}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of sentences in each scene: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{sentence\PYZus{}count\PYZus{}scene}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{sentences} \PY{o}{=} \PY{p}{[}\PY{n}{sentence} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{scene}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of lines: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentences}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{word\PYZus{}count\PYZus{}sentence} \PY{o}{=} \PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{sentences}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of words in each line: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{word\PYZus{}count\PYZus{}sentence}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The sentences }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ to }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{o}{*}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Dataset Stats
Roughly the number of unique words: 11492
Number of scenes: 262
Average number of sentences in each scene: 15.248091603053435
Number of lines: 4257
Average number of words in each line: 11.50434578341555

The sentences 0 to 10:
Moe\_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.
Bart\_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.
Moe\_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?
Moe\_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.
Moe\_Szyslak: What's the matter Homer? You're not your normal effervescent self.
Homer\_Simpson: I got my problems, Moe. Give me another one.
Moe\_Szyslak: Homer, hey, you should not drink to forget your problems.
Barney\_Gumble: Yeah, you should only drink to enhance your social skills.



    \end{Verbatim}

    \subsection{Implement Preprocessing
Functions}\label{implement-preprocessing-functions}

The first thing to do to any dataset is preprocessing. Implement the
following preprocessing functions below: - Lookup Table - Tokenize
Punctuation

\subsubsection{Lookup Table}\label{lookup-table}

To create a word embedding, you first need to transform the words to
ids. In this function, create two dictionaries: - Dictionary to go from
the words to an id, we'll call \texttt{vocab\_to\_int} - Dictionary to
go from the id to word, we'll call \texttt{int\_to\_vocab}

Return these dictionaries in the following tuple
\texttt{(vocab\_to\_int,\ int\_to\_vocab)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{Counter}
        
        \PY{k}{def} \PY{n+nf}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create lookup tables for vocabulary}
        \PY{l+s+sd}{    :param text: The text of tv scripts split into words}
        \PY{l+s+sd}{    :return: A tuple of dicts (vocab\PYZus{}to\PYZus{}int, int\PYZus{}to\PYZus{}vocab)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{counts} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{text}\PY{p}{)}
            \PY{n}{vocab} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{counts}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{counts}\PY{o}{.}\PY{n}{get}\PY{p}{,} \PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
            \PY{n}{vocab\PYZus{}to\PYZus{}int} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{word}\PY{p}{:} \PY{n}{ii} \PY{k}{for} \PY{n}{ii}\PY{p}{,} \PY{n}{word} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{\PYZcb{}}
            
            \PY{n}{int\PYZus{}to\PYZus{}vocab} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{v}\PY{p}{:} \PY{n}{k} \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
        
            \PY{k}{return} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}
        
        \PY{c+c1}{\PYZsh{} \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{c+c1}{\PYZsh{} \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/ubuntu/anaconda3/envs/tensorflow\_p36/lib/python3.6/site-packages/h5py/\_\_init\_\_.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from .\_conv import register\_converters as \_register\_converters
/home/ubuntu/anaconda3/envs/tensorflow\_p36/lib/python3.6/site-packages/matplotlib/\_\_init\_\_.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line \#2
  (fname, cnt))
/home/ubuntu/anaconda3/envs/tensorflow\_p36/lib/python3.6/site-packages/matplotlib/\_\_init\_\_.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line \#3
  (fname, cnt))

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Tokenize Punctuation}\label{tokenize-punctuation}

We'll be splitting the script into a word array using spaces as
delimiters. However, punctuations like periods and exclamation marks
make it hard for the neural network to distinguish between the word
"bye" and "bye!".

Implement the function \texttt{token\_lookup} to return a dict that will
be used to tokenize symbols like "!" into
"\textbar{}\textbar{}Exclamation\_Mark\textbar{}\textbar{}". Create a
dictionary for the following symbols where the symbol is the key and
value is the token: - Period ( . ) - Comma ( , ) - Quotation Mark ( " )
- Semicolon ( ; ) - Exclamation mark ( ! ) - Question mark ( ? ) - Left
Parentheses ( ( ) - Right Parentheses ( ) ) - Dash ( -\/- ) - Return (
\n )

This dictionary will be used to token the symbols and add the delimiter
(space) around it. This separates the symbols as it's own word, making
it easier for the neural network to predict on the next word. Make sure
you don't use a token that could be confused as a word. Instead of using
the token "dash", try using something like
"\textbar{}\textbar{}dash\textbar{}\textbar{}".

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{token\PYZus{}lookup}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Generate a dict to turn punctuation into a token.}
        \PY{l+s+sd}{    :return: Tokenize dictionary where the key is the punctuation and the value is the token}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{n}{token} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Period||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Comma||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Quotation\PYZus{}Mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{;}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Semicolon||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{!}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Exclamation\PYZus{}Mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Question\PYZus{}Mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Left\PYZus{}Parenthesis||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Right\PYZus{}Parenthesis||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Dash||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||Return||}\PY{l+s+s1}{\PYZsq{}}
                    \PY{p}{\PYZcb{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{n}{token}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}tokenize}\PY{p}{(}\PY{n}{token\PYZus{}lookup}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsection{Preprocess all the data and save
it}\label{preprocess-all-the-data-and-save-it}

Running the code cell below will preprocess all the data and save it to
file.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Preprocess Training, Validation, and Testing Data}
        \PY{n}{helper}\PY{o}{.}\PY{n}{preprocess\PYZus{}and\PYZus{}save\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{,} \PY{n}{token\PYZus{}lookup}\PY{p}{,} \PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \section{Check Point}\label{check-point}

This is your first checkpoint. If you ever decide to come back to this
notebook or have to restart the notebook, you can start from here. The
preprocessed data has been saved to disk.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        
        \PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \subsection{Build the Neural Network}\label{build-the-neural-network}

You'll build the components necessary to build a RNN by implementing the
following functions below: - get\_inputs - get\_init\_cell - get\_embed
- build\_rnn - build\_nn - get\_batches

\subsubsection{Check the Version of TensorFlow and Access to
GPU}\label{check-the-version-of-tensorflow-and-access-to-gpu}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{from} \PY{n+nn}{distutils}\PY{n+nn}{.}\PY{n+nn}{version} \PY{k}{import} \PY{n}{LooseVersion}
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        
        \PY{c+c1}{\PYZsh{} Check TensorFlow Version}
        \PY{k}{assert} \PY{n}{LooseVersion}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{LooseVersion}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Please use TensorFlow version 1.3 or newer}\PY{l+s+s1}{\PYZsq{}}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow Version: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
        
        
        \PY{c+c1}{\PYZsh{} Check for a GPU}
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{warnings}\PY{o}{.}\PY{n}{warn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No GPU found. Please use a GPU to train your neural network.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Default GPU Device: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TensorFlow Version: 1.6.0
Default GPU Device: /device:GPU:0

    \end{Verbatim}

    \subsubsection{Input}\label{input}

Implement the \texttt{get\_inputs()} function to create TF Placeholders
for the Neural Network. It should create the following placeholders: -
Input text placeholder named "input" using the
\href{https://www.tensorflow.org/api_docs/python/tf/placeholder}{TF
Placeholder} \texttt{name} parameter. - Targets placeholder - Learning
Rate placeholder

Return the placeholders in the following tuple
\texttt{(Input,\ Targets,\ LearningRate)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Create TF Placeholders for input, targets, and learning rate.}
         \PY{l+s+sd}{    :return: Tuple (input, targets, learning rate)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n+nb}{input} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{int32}\PY{p}{,} \PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{targets} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{int32}\PY{p}{,} \PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{targets}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n+nb}{input}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{learning\PYZus{}rate}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}inputs}\PY{p}{(}\PY{n}{get\PYZus{}inputs}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Build RNN Cell and
Initialize}\label{build-rnn-cell-and-initialize}

Stack one or more
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell}{\texttt{BasicLSTMCells}}
in a
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell}{\texttt{MultiRNNCell}}.
- The Rnn size should be set using \texttt{rnn\_size} - Initalize Cell
State using the MultiRNNCell's
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell\#zero_state}{\texttt{zero\_state()}}
function - Apply the name "initial\_state" to the initial state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the cell and initial state in the following tuple
\texttt{(Cell,\ InitialState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Create an RNN Cell and initialize it.}
         \PY{l+s+sd}{    :param batch\PYZus{}size: Size of batches}
         \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of RNNs}
         \PY{l+s+sd}{    :return: Tuple (cell, initialize state)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{n\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{;}
             
             \PY{n}{cell} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{rnn}\PY{o}{.}\PY{n}{BasicLSTMCell}\PY{p}{(}\PY{n}{rnn\PYZus{}size}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} NOTE\PYZus{}TO\PYZus{}MYSELF: Aren\PYZsq{}t we supposed to add dropout layers here?}
             \PY{n}{cell} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{rnn}\PY{o}{.}\PY{n}{MultiRNNCell}\PY{p}{(}\PY{p}{[}\PY{n}{cell}\PY{p}{]} \PY{o}{*} \PY{n}{n\PYZus{}layers}\PY{p}{)}
         
             \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{cell}\PY{o}{.}\PY{n}{zero\PYZus{}state}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
             \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{n+nb}{input}\PY{o}{=}\PY{n}{initial\PYZus{}state}\PY{p}{,}\PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{initial\PYZus{}state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n}{cell}\PY{p}{,} \PY{n}{initial\PYZus{}state}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Word Embedding}\label{word-embedding}

Apply embedding to \texttt{input\_data} using TensorFlow. Return the
embedded sequence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}embed}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Create embedding for \PYZlt{}input\PYZus{}data\PYZgt{}.}
         \PY{l+s+sd}{    :param input\PYZus{}data: TF placeholder for text input.}
         \PY{l+s+sd}{    :param vocab\PYZus{}size: Number of words in vocabulary.}
         \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
         \PY{l+s+sd}{    :return: Embedded input.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{embedding} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{random\PYZus{}uniform}\PY{p}{(}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{embed} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{embedding\PYZus{}lookup}\PY{p}{(}\PY{n}{embedding}\PY{p}{,} \PY{n}{input\PYZus{}data}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n}{embed}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}embed}\PY{p}{(}\PY{n}{get\PYZus{}embed}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Build RNN}\label{build-rnn}

You created a RNN Cell in the \texttt{get\_init\_cell()} function. Time
to use the cell to create a RNN. - Build the RNN using the
\href{https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn}{\texttt{tf.nn.dynamic\_rnn()}}
- Apply the name "final\_state" to the final state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the outputs and final\_state state in the following tuple
\texttt{(Outputs,\ FinalState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k}{def} \PY{n+nf}{build\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Create a RNN using a RNN Cell}
         \PY{l+s+sd}{    :param cell: RNN Cell}
         \PY{l+s+sd}{    :param inputs: Input text data}
         \PY{l+s+sd}{    :return: Tuple (Outputs, Final State)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{outputs}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{dynamic\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
             
             \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{n+nb}{input}\PY{o}{=}\PY{n}{final\PYZus{}state}\PY{p}{,}\PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{final\PYZus{}state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n}{outputs}\PY{p}{,} \PY{n}{final\PYZus{}state}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}rnn}\PY{p}{(}\PY{n}{build\PYZus{}rnn}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Build the Neural Network}\label{build-the-neural-network}

Apply the functions you implemented above to: - Apply embedding to
\texttt{input\_data} using your
\texttt{get\_embed(input\_data,\ vocab\_size,\ embed\_dim)} function. -
Build RNN using \texttt{cell} and your
\texttt{build\_rnn(cell,\ inputs)} function. - Apply a fully connected
layer with a linear activation and \texttt{vocab\_size} as the number of
outputs.

Return the logits and final state in the following tuple (Logits,
FinalState)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k}{def} \PY{n+nf}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Build part of the neural network}
         \PY{l+s+sd}{    :param cell: RNN cell}
         \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of rnns}
         \PY{l+s+sd}{    :param input\PYZus{}data: Input data}
         \PY{l+s+sd}{    :param vocab\PYZus{}size: Vocabulary size}
         \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
         \PY{l+s+sd}{    :return: Tuple (Logits, FinalState)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{n}{inputs} \PY{o}{=} \PY{n}{get\PYZus{}embed}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}
             \PY{n}{outputs}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{build\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} lstm\PYZus{}output = tf.contrib.layers.fully\PYZus{}connected(outputs[:, \PYZhy{}1], vocab\PYZus{}size, activation\PYZus{}fn=None)}
             
             \PY{c+c1}{\PYZsh{} seq\PYZus{}output = tf.concat(lstm\PYZus{}output, axis=1)}
             \PY{c+c1}{\PYZsh{} x = tf.reshape(seq\PYZus{}output, [\PYZhy{}1, rnn\PYZus{}size])}
             
             \PY{c+c1}{\PYZsh{} with tf.variable\PYZus{}scope(\PYZsq{}softmax\PYZsq{}):}
             \PY{c+c1}{\PYZsh{}    softmax\PYZus{}w = tf.Variable(tf.truncated\PYZus{}normal((rnn\PYZus{}size, vocab\PYZus{}size\PYZus{}size), stddev=0.1))}
             \PY{c+c1}{\PYZsh{}    softmax\PYZus{}b = tf.Variable(tf.zeros(out\PYZus{}size))}
                 
             \PY{c+c1}{\PYZsh{} logits = tf.matmul(x, softmax\PYZus{}w) + softmax\PYZus{}b}
             
             \PY{c+c1}{\PYZsh{} Use softmax to get the probabilities for predicted characters}
             \PY{c+c1}{\PYZsh{} predictionas = tf.nn.softmax(logits, name=\PYZsq{}predictions\PYZsq{})}
             
             \PY{n}{logits}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{fully\PYZus{}connected}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{activation\PYZus{}fn}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} 
                                                      \PY{n}{weights\PYZus{}initializer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{truncated\PYZus{}normal\PYZus{}initializer}\PY{p}{(}\PY{n}{stddev}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
                                                      \PY{n}{biases\PYZus{}initializer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{zeros\PYZus{}initializer}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n}{logits}\PY{p}{,} \PY{n}{final\PYZus{}state}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}nn}\PY{p}{(}\PY{n}{build\PYZus{}nn}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsubsection{Batches}\label{batches}

Implement \texttt{get\_batches} to create batches of input and targets
using \texttt{int\_text}. The batches should be a Numpy array with the
shape
\texttt{(number\ of\ batches,\ 2,\ batch\ size,\ sequence\ length)}.
Each batch contains two elements: - The first element is a single batch
of \textbf{input} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}} - The second element is a
single batch of \textbf{targets} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}}

If you can't fill the last batch with enough data, drop the last batch.

For example,
\texttt{get\_batches({[}1,\ 2,\ 3,\ 4,\ 5,\ 6,\ 7,\ 8,\ 9,\ 10,\ 11,\ 12,\ 13,\ 14,\ 15,\ 16,\ 17,\ 18,\ 19,\ 20{]},\ 3,\ 2)}
would return a Numpy array of the following:

\begin{verbatim}
[
  # First Batch
  [
    # Batch of Input
    [[ 1  2], [ 7  8], [13 14]]
    # Batch of targets
    [[ 2  3], [ 8  9], [14 15]]
  ]

  # Second Batch
  [
    # Batch of Input
    [[ 3  4], [ 9 10], [15 16]]
    # Batch of targets
    [[ 4  5], [10 11], [16 17]]
  ]

  # Third Batch
  [
    # Batch of Input
    [[ 5  6], [11 12], [17 18]]
    # Batch of targets
    [[ 6  7], [12 13], [18  1]]
  ]
]
\end{verbatim}

Notice that the last target value in the last batch is the first input
value of the first batch. In this case, \texttt{1}. This is a common
technique used when creating sequence batches, although it is rather
unintuitive.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k}{def} \PY{n+nf}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Return batches of input and target}
         \PY{l+s+sd}{    :param int\PYZus{}text: Text with the words replaced by their ids}
         \PY{l+s+sd}{    :param batch\PYZus{}size: The size of batch}
         \PY{l+s+sd}{    :param seq\PYZus{}length: The length of sequence}
         \PY{l+s+sd}{    :return: Batches as a Numpy array}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{chars\PYZus{}per\PYZus{}batch} \PY{o}{=} \PY{n}{batch\PYZus{}size} \PY{o}{*} \PY{n}{seq\PYZus{}length}
             \PY{n}{n\PYZus{}batches} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{n}{chars\PYZus{}per\PYZus{}batch}
         
             \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{n}{chars\PYZus{}per\PYZus{}batch} \PY{o}{*} \PY{n}{n\PYZus{}batches}\PY{p}{]} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
             \PY{n}{batches} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}batches}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{batch\PYZus{}start} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}batches}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} batches[i] = np.array()}
                 \PY{n}{batch\PYZus{}start} \PY{o}{=} \PY{n}{i} \PY{o}{*} \PY{n}{seq\PYZus{}length}
                 \PY{n}{start} \PY{o}{=} \PY{n}{batch\PYZus{}start}
                 \PY{k}{for} \PY{n}{ii} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                     \PY{n}{inputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{[}\PY{n}{start}\PY{p}{:} \PY{n}{start} \PY{o}{+} \PY{n}{seq\PYZus{}length}\PY{p}{]}\PY{p}{)}
                     \PY{n}{outputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{[}\PY{n}{start} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{:} \PY{n}{start} \PY{o}{+} \PY{l+m+mi}{1} \PY{o}{+} \PY{n}{seq\PYZus{}length}\PY{p}{]}\PY{p}{)}
         
                     \PY{n}{batches}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{ii}\PY{p}{]} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{n}{start}\PY{p}{:} \PY{n}{start} \PY{o}{+} \PY{n}{seq\PYZus{}length}\PY{p}{]} \PY{c+c1}{\PYZsh{} inputs}
                     \PY{n}{batches}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{ii}\PY{p}{]} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{n}{start} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{:} \PY{n}{start} \PY{o}{+} \PY{l+m+mi}{1} \PY{o}{+} \PY{n}{seq\PYZus{}length}\PY{p}{]} \PY{c+c1}{\PYZsh{} outputs}
         
                     \PY{n}{start} \PY{o}{+}\PY{o}{=} \PY{n}{n\PYZus{}batches} \PY{o}{*} \PY{n}{seq\PYZus{}length}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{k}{return} \PY{n}{batches}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}batches}\PY{p}{(}\PY{n}{get\PYZus{}batches}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \subsection{Neural Network Training}\label{neural-network-training}

\subsubsection{Hyperparameters}\label{hyperparameters}

Tune the following parameters:

\begin{itemize}
\tightlist
\item
  Set \texttt{num\_epochs} to the number of epochs.
\item
  Set \texttt{batch\_size} to the batch size.
\item
  Set \texttt{rnn\_size} to the size of the RNNs.
\item
  Set \texttt{embed\_dim} to the size of the embedding.
\item
  Set \texttt{seq\_length} to the length of sequence.
\item
  Set \texttt{learning\_rate} to the learning rate.
\item
  Set \texttt{show\_every\_n\_batches} to the number of batches the
  neural network should print progress.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{} Number of Epochs}
         \PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{50}
         \PY{c+c1}{\PYZsh{} Batch Size}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{c+c1}{\PYZsh{} RNN Size}
         \PY{n}{rnn\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{256}
         \PY{c+c1}{\PYZsh{} Embedding Dimension Size}
         \PY{n}{embed\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{300}
         \PY{c+c1}{\PYZsh{} Sequence Length}
         \PY{n}{seq\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{12}
         \PY{c+c1}{\PYZsh{} Learning Rate}
         \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.001}
         \PY{c+c1}{\PYZsh{} Show stats for every n number of batches}
         \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{=} \PY{l+m+mi}{50}
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{save\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./save}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \subsubsection{Build the Graph}\label{build-the-graph}

Build the graph using the neural network you implemented.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{contrib} \PY{k}{import} \PY{n}{seq2seq}
         
         \PY{n}{train\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
         \PY{k}{with} \PY{n}{train\PYZus{}graph}\PY{o}{.}\PY{n}{as\PYZus{}default}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{vocab\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
             \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{n}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}
             \PY{n}{input\PYZus{}data\PYZus{}shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{input\PYZus{}text}\PY{p}{)}
             \PY{n}{cell}\PY{p}{,} \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}
             \PY{n}{logits}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Probabilities for generating words}
             \PY{n}{probs} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{logits}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{probs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Loss function}
             \PY{n}{cost} \PY{o}{=} \PY{n}{seq2seq}\PY{o}{.}\PY{n}{sequence\PYZus{}loss}\PY{p}{(}
                 \PY{n}{logits}\PY{p}{,}
                 \PY{n}{targets}\PY{p}{,}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Optimizer}
             \PY{n}{optimizer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{AdamOptimizer}\PY{p}{(}\PY{n}{lr}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Gradient Clipping}
             \PY{n}{gradients} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{compute\PYZus{}gradients}\PY{p}{(}\PY{n}{cost}\PY{p}{)}
             \PY{n}{capped\PYZus{}gradients} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{clip\PYZus{}by\PYZus{}value}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} \PY{n}{var}\PY{p}{)} \PY{k}{for} \PY{n}{grad}\PY{p}{,} \PY{n}{var} \PY{o+ow}{in} \PY{n}{gradients} \PY{k}{if} \PY{n}{grad} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}
             \PY{n}{train\PYZus{}op} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{apply\PYZus{}gradients}\PY{p}{(}\PY{n}{capped\PYZus{}gradients}\PY{p}{)}
\end{Verbatim}


    \subsection{Train}\label{train}

Train the neural network on the preprocessed data. If you have a hard
time getting a good loss, check the
\href{https://discussions.udacity.com/}{forums} to see if anyone is
having the same problem.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{batches} \PY{o}{=} \PY{n}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}
         
         \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{train\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
             \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{global\PYZus{}variables\PYZus{}initializer}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{for} \PY{n}{epoch\PYZus{}i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
                 \PY{n}{state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{batches}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
         
                 \PY{k}{for} \PY{n}{batch\PYZus{}i}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{:}
                     \PY{n}{feed} \PY{o}{=} \PY{p}{\PYZob{}}
                         \PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{x}\PY{p}{,}
                         \PY{n}{targets}\PY{p}{:} \PY{n}{y}\PY{p}{,}
                         \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{state}\PY{p}{,}
                         \PY{n}{lr}\PY{p}{:} \PY{n}{learning\PYZus{}rate}\PY{p}{\PYZcb{}}
                     \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{p}{[}\PY{n}{cost}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{train\PYZus{}op}\PY{p}{]}\PY{p}{,} \PY{n}{feed}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} Show every \PYZlt{}show\PYZus{}every\PYZus{}n\PYZus{}batches\PYZgt{} batches}
                     \PY{k}{if} \PY{p}{(}\PY{n}{epoch\PYZus{}i} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)} \PY{o}{+} \PY{n}{batch\PYZus{}i}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}:\PYZgt{}3\PYZcb{}}\PY{l+s+s1}{ Batch }\PY{l+s+si}{\PYZob{}:\PYZgt{}4\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{   train\PYZus{}loss = }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                             \PY{n}{epoch\PYZus{}i}\PY{p}{,}
                             \PY{n}{batch\PYZus{}i}\PY{p}{,}
                             \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{,}
                             \PY{n}{train\PYZus{}loss}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Save Model}
             \PY{n}{saver} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{Saver}\PY{p}{(}\PY{p}{)}
             \PY{n}{saver}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Trained and Saved}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch   0 Batch    0/57   train\_loss = 8.838
Epoch   0 Batch   50/57   train\_loss = 5.844
Epoch   1 Batch   43/57   train\_loss = 5.335
Epoch   2 Batch   36/57   train\_loss = 5.039
Epoch   3 Batch   29/57   train\_loss = 4.791
Epoch   4 Batch   22/57   train\_loss = 4.733
Epoch   5 Batch   15/57   train\_loss = 4.414
Epoch   6 Batch    8/57   train\_loss = 4.429
Epoch   7 Batch    1/57   train\_loss = 4.353
Epoch   7 Batch   51/57   train\_loss = 4.205
Epoch   8 Batch   44/57   train\_loss = 4.074
Epoch   9 Batch   37/57   train\_loss = 3.895
Epoch  10 Batch   30/57   train\_loss = 3.992
Epoch  11 Batch   23/57   train\_loss = 3.695
Epoch  12 Batch   16/57   train\_loss = 3.595
Epoch  13 Batch    9/57   train\_loss = 3.474
Epoch  14 Batch    2/57   train\_loss = 3.337
Epoch  14 Batch   52/57   train\_loss = 3.421
Epoch  15 Batch   45/57   train\_loss = 3.202
Epoch  16 Batch   38/57   train\_loss = 3.102
Epoch  17 Batch   31/57   train\_loss = 2.991
Epoch  18 Batch   24/57   train\_loss = 2.973
Epoch  19 Batch   17/57   train\_loss = 2.974
Epoch  20 Batch   10/57   train\_loss = 2.805
Epoch  21 Batch    3/57   train\_loss = 2.690
Epoch  21 Batch   53/57   train\_loss = 2.588
Epoch  22 Batch   46/57   train\_loss = 2.655
Epoch  23 Batch   39/57   train\_loss = 2.511
Epoch  24 Batch   32/57   train\_loss = 2.392
Epoch  25 Batch   25/57   train\_loss = 2.378
Epoch  26 Batch   18/57   train\_loss = 2.278
Epoch  27 Batch   11/57   train\_loss = 2.157
Epoch  28 Batch    4/57   train\_loss = 2.091
Epoch  28 Batch   54/57   train\_loss = 2.204
Epoch  29 Batch   47/57   train\_loss = 2.084
Epoch  30 Batch   40/57   train\_loss = 2.073
Epoch  31 Batch   33/57   train\_loss = 1.937
Epoch  32 Batch   26/57   train\_loss = 1.821
Epoch  33 Batch   19/57   train\_loss = 1.733
Epoch  34 Batch   12/57   train\_loss = 1.698
Epoch  35 Batch    5/57   train\_loss = 1.692
Epoch  35 Batch   55/57   train\_loss = 1.631
Epoch  36 Batch   48/57   train\_loss = 1.702
Epoch  37 Batch   41/57   train\_loss = 1.654
Epoch  38 Batch   34/57   train\_loss = 1.538
Epoch  39 Batch   27/57   train\_loss = 1.500
Epoch  40 Batch   20/57   train\_loss = 1.426
Epoch  41 Batch   13/57   train\_loss = 1.504
Epoch  42 Batch    6/57   train\_loss = 1.272
Epoch  42 Batch   56/57   train\_loss = 1.442
Epoch  43 Batch   49/57   train\_loss = 1.251
Epoch  44 Batch   42/57   train\_loss = 1.249
Epoch  45 Batch   35/57   train\_loss = 1.218
Epoch  46 Batch   28/57   train\_loss = 1.172
Epoch  47 Batch   21/57   train\_loss = 1.166
Epoch  48 Batch   14/57   train\_loss = 1.141
Epoch  49 Batch    7/57   train\_loss = 1.144
Model Trained and Saved

    \end{Verbatim}

    \subsection{Save Parameters}\label{save-parameters}

Save \texttt{seq\_length} and \texttt{save\_dir} for generating a new TV
script.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{c+c1}{\PYZsh{} Save parameters for checkpoint}
         \PY{n}{helper}\PY{o}{.}\PY{n}{save\PYZus{}params}\PY{p}{(}\PY{p}{(}\PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \section{Checkpoint}\label{checkpoint}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{helper}
         \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
         
         \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
         \PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{load\PYZus{}dir} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \subsection{Implement Generate
Functions}\label{implement-generate-functions}

\subsubsection{Get Tensors}\label{get-tensors}

Get tensors from \texttt{loaded\_graph} using the function
\href{https://www.tensorflow.org/api_docs/python/tf/Graph\#get_tensor_by_name}{\texttt{get\_tensor\_by\_name()}}.
Get the tensors using the following names: - "input:0" -
"initial\_state:0" - "final\_state:0" - "probs:0"

Return the tensors in the following tuple
\texttt{(InputTensor,\ InitialStateTensor,\ FinalStateTensor,\ ProbsTensor)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Get input, initial state, final state, and probabilities tensor from \PYZlt{}loaded\PYZus{}graph\PYZgt{}}
        \PY{l+s+sd}{    :param loaded\PYZus{}graph: TensorFlow graph loaded from file}
        \PY{l+s+sd}{    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}tensors}\PY{p}{(}\PY{n}{get\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Choose Word}\label{choose-word}

Implement the \texttt{pick\_word()} function to select the next word
using \texttt{probabilities}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Pick the next word in the generated text}
        \PY{l+s+sd}{    :param probabilities: Probabilites of the next word}
        \PY{l+s+sd}{    :param int\PYZus{}to\PYZus{}vocab: Dictionary of word ids as the keys and words as the values}
        \PY{l+s+sd}{    :return: String of the predicted word}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}pick\PYZus{}word}\PY{p}{(}\PY{n}{pick\PYZus{}word}\PY{p}{)}
\end{Verbatim}


    \subsection{Generate TV Script}\label{generate-tv-script}

This will generate the TV script for you. Set \texttt{gen\_length} to
the length of TV script you want to generate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{gen\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{200}
        \PY{c+c1}{\PYZsh{} homer\PYZus{}simpson, moe\PYZus{}szyslak, or Barney\PYZus{}Gumble}
        \PY{n}{prime\PYZus{}word} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{moe\PYZus{}szyslak}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{loaded\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
        \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{loaded\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Load saved model}
            \PY{n}{loader} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{import\PYZus{}meta\PYZus{}graph}\PY{p}{(}\PY{n}{load\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.meta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{loader}\PY{o}{.}\PY{n}{restore}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{load\PYZus{}dir}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Get Tensors from loaded model}
            \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{probs} \PY{o}{=} \PY{n}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Sentences generation setup}
            \PY{n}{gen\PYZus{}sentences} \PY{o}{=} \PY{p}{[}\PY{n}{prime\PYZus{}word} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
            \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Generate sentences}
            \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{gen\PYZus{}length}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Dynamic Input}
                \PY{n}{dyn\PYZus{}input} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{gen\PYZus{}sentences}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{seq\PYZus{}length}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{]}
                \PY{n}{dyn\PYZus{}seq\PYZus{}length} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dyn\PYZus{}input}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Get Prediction}
                \PY{n}{probabilities}\PY{p}{,} \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}
                    \PY{p}{[}\PY{n}{probs}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{]}\PY{p}{,}
                    \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{dyn\PYZus{}input}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{prev\PYZus{}state}\PY{p}{\PYZcb{}}\PY{p}{)}
                
                \PY{n}{pred\PYZus{}word} \PY{o}{=} \PY{n}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{dyn\PYZus{}seq\PYZus{}length}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
        
                \PY{n}{gen\PYZus{}sentences}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pred\PYZus{}word}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Remove tokens}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{gen\PYZus{}sentences}\PY{p}{)}
            \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{token} \PY{o+ow}{in} \PY{n}{token\PYZus{}dict}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                \PY{n}{ending} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
                \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{token}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{p}{)}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{( }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{tv\PYZus{}script}\PY{p}{)}
\end{Verbatim}


    \section{The TV Script is
Nonsensical}\label{the-tv-script-is-nonsensical}

It's ok if the TV script doesn't make any sense. We trained on less than
a megabyte of text. In order to get good results, you'll have to use a
smaller vocabulary or get more data. Luckily there's more data! As we
mentioned in the beggining of this project, this is a subset of
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{another
dataset}. We didn't have you train on all the data, because that would
take too long. However, you are free to train your neural network on all
the data. After you complete the project, of course. \# Submitting This
Project When submitting this project, make sure to run all the cells
before saving the notebook. Save the notebook file as
"dlnd\_tv\_script\_generation.ipynb" and save it as a HTML file under
"File" -\textgreater{} "Download as". Include the "helper.py" and
"problem\_unittests.py" files in your submission.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
